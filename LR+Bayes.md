[TOC]

## LR

####方法

分类模型，形式为参数化的逻辑斯谛分布。逻辑回归假设因变量 y 服从伯努利分布 。逻辑回归通过sigmoid函数引入非线性因素。

逻辑回归是判别式模型，对条件概率分布建模。

优点
预测结果是界于0和1之间的概率；可以适用于连续性和类别性自变量；容易使用和解释；适用于大规模数据集

缺点

容易欠拟合，分类精度不高；数据特征有缺失或者特征空间很大时表现效果并不好。 

####公式

$$
P(Y=1|X)=\frac{exp(WX+b)}{1+exp(WX+b)}
$$

$$
P(Y=0|X)=\frac{1}{1+exp(WX+b)}
$$

其中W，X都是多元参数和变量。逻辑斯谛回归模型将线性函数（$WX$）转换为概率。

**几率/优势比：**该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率为$p$，那么该事件发生的几率是$\frac{p}{1-p}$，该事件发生的对数几率或logit函数是
$$
logit(p)=log\frac{p}{1-p}
$$
对逻辑回归，对数几率或logit函数是
$$
log\frac{P(Y=1|X)}{1-P(Y=1|X)}=WX
$$

#### 训练过程

使用极大似然估计法估计模型参数。

设$P(Y=1|x)=\pi(x)$，$P(Y=0|x)=1-\pi(x)$，似然函数（将每个样本的概率值乘起来）为
$$
\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$
其中N为训练样本个数，对数似然函数为
$$
L(w)=\sum_{i=1}^N[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i))]\\
=\sum_{i=1}^N[y_i(wx_i)-log(1+exp(wx_i))]
$$
对$L(w)$求极大值得到$w$的估计值。(对$w$求偏导，使用梯度下降法或拟牛顿法)

**LR梯度求解过程：**

Logistic (sigmoid)函数对参数求导得：
$$
\frac{\partial \pi(x)}{\partial w}=\frac{\partial (\frac{exp(wx+b)}{1+exp(wx+b)})}{\partial w}=x*\frac{exp(wx+b)}{1+exp(wx+b)}*\frac{1}{1+exp(wx+b)}=x*\pi(x)*(1-\pi(x))
$$
对数似然函数对Logistic函数求导：
$$
\frac{\partial L(w)}{\partial \pi(x)}=\sum_{i=1}^N[\frac{y_i}{\pi(x)}-\frac{1-y_i}{1-\pi(x)}]
$$
由此可以推导出对数似然函数对参数w的梯度：
$$
\begin{aligned}
\frac{\partial L(w)}{\partial w}&=\frac{\partial L(w)}{\partial \pi(x)}*\frac{\partial \pi(x)}{\partial w}\\
&=\sum_{i=1}^N[(\frac{y_i}{\pi(x)}-\frac{1-y_i}{1-\pi(x)})*x*\pi(x)*(1-\pi(x))]\\
&=\sum_{i=1}^N[y_ix_i(1-\pi(x_i))-(1-y_i)x_i\pi(x_i)]\\
&=\sum_{i=1}^Nx_i(y_i-\pi(x_i))
\end{aligned}
$$


#### 多项逻辑回归

用于多类分类，$Y$的取值从1到K。
$$
P(Y=k|X)=\frac{exp(W_kX)}{1+\sum_{k=1}^{K-1}exp(W_kX)}，k=1,2,...,K-1
$$

$$
P(Y=K|X)=\frac{1}{1+\sum_{k=1}^{K-1}exp(W_kX)}
$$



## Bayes

#### 方法

基于**贝叶斯定理**和**条件独立性假设**，是一个分类方法。条件独立性假设：用于分类的特征在类确定的条件下都是条件独立的。

测算未知样本的后验概率，以最大后验概率对应的类别作为预测的类别。

朴素贝叶斯是生成式模型，对联合概率分布建模。

优点：运算简单高效，分类效率稳定，对缺失值和异常值不太敏感 ，适用于数据少的情况；

缺点：对输入数据的表达形式（离散、连续，值极大极小之类的） 比较敏感，模型的前提条件（特征独立）在实际中很难满足，先验概率取决于假设 。

#### 推导公式

1、条件概率公式：
$$
P(B|A)=\frac{P(AB)}{P(A)}
$$
2、全概率公式：
$$
P(A)=\sum_{i=1}^{n}P(AB_i)=\sum_{i=1}^{n}P(B_i)P(A|B_i)
$$
3、条件概率乘法公式：
$$
P(AB)=P(A)P(B|A)=P(B)P(A|B)
$$


4、贝叶斯公式：
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$
5、朴素贝叶斯分类器：

贝叶斯分类器的核心是计算在已知输入$X=x$的情况下，样本属于某个类别$C_i$的概率 。
$$
P(C_i|X)=\frac{P(C_iX)}{P(X)}=\frac{P(C_i)P(X|C_i)}{\sum_{i=1}^kP(C_i)P(X|C_i)}
$$
分母$\sum_{i=1}^kP(C_i)P(X|C_i)$是常量，实际表示$P(X)$，即输入；$P(C_i)$一般以$C_i$类别出现的频率代替，为先验概率；后验概率最大化即为使$P(X|C_i)$最大。

特征条件独立假设：
$$
P(X|C_i)=P(x_1,x_2,...,x_p|C_i)=P(x_1|C_i)P(x_2|C_i)...P(x_p|C_i)
$$
最后，朴素贝叶斯分类器可以表示为：
$$
y=f(x)=arg\max_{c_k}P(Y=c_k)\prod_jP(X=x_j|Y=c_k)
$$
选取使右式最大的$c_k$的值为当前输入$X$的类别。

#### 训练过程

1、利用训练集计算每个$i$和$j$对应的$P(C_i)$和$P(x_j|C_i)$的值，$x_j$是输入的特征之一；

2、根据给定的未知类别的实例$X=(x_1,x_2,...,x_p)$，计算$P(Y=c_k)\prod_jP(X=x_j|Y=c_k)$，选择最大值对应的$c_k$作为预测类别。

#### 其他贝叶斯分类器

1、**普通贝叶斯分类器**，以指示函数（对应特征出现频率）计算概率。
$$
P(x_j|C_i)=\frac{\sum_{i=1}^NI(X=x_j,Y=C_i)}{\sum_{i=1}^NI(Y=C_i)}
$$
2、**高斯贝叶斯分类器**，自变量X为连续的数值型，假设自变量X服从高斯正态分布。
$$
P(x_j|C_i)=\frac{1}{\sqrt{2\pi}\sigma_{ji}}exp(-\frac{(x_j-\mu_{ji})^2}{2\sigma_{ji}^2})
$$
其中$x_j$为第$j$个自变量的取值，$\mu_{ji}$为训练集中自变量$x_j$属于类别$C_i$的均值，$\sigma_{ji}$为训练集中自变量$x_j$属于类别$C_i$的标准差。 

3、**多项式贝叶斯分类器**，自变量X为离散型变量，假设自变量X的条件概率服从多项式分布。
$$
P(x_j=x_{jk}|C_i)=\frac{N_{ik}+\alpha}{N_i+n\alpha}
$$
其中$x_{jk}$为第$j$个自变量$x_j$的取值，$N_{ik}$表示类别为$C_i$时$x_j$取值为$x_{jk}$的样本个数，$N_i$为类别$C_i$的样本个数，$\alpha$为平滑系数（通常为1，防止概率为0）。

4、伯努利贝叶斯分类器，自变量X均为0-1二元变量，假设自变量X的条件概率服从伯努利分布 。
$$
P(x_j|C_i)=px_j+(1-p)(1-x_j)
$$

$$
p=P(x_j=1|C_i)=\frac{N_{i1}+\alpha}{N_i+n\alpha}
$$

其中$x_{jk}$为第$j$个自变量$x_j$的取值，$N_{i1}$表示类别为$C_i$时$x_j$取值为1的样本个数，$N_i$为类别$C_i$的样本个数，$\alpha$为平滑系数（通常为1，防止概率为0）。





