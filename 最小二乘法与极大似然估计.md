#### **线性回归问题中，当模型的估计值和真实值之间的误差服从均值为0的高斯分布时，对模型参数的最小二乘估计和最大似然估计等价。**



## 最小二乘法

线性回归问题中，构造出一个使目标点都尽可能出现在线上的函数。

**公式：**
$$
J(y)=min_\theta(\frac{1}{N}\sum_{i=1}^N||y_i-\hat{y}||_2^2)
$$
在Linear Regression中，前提假设是$y$服从正态分布。

## 最大似然估计

令$p_{model}(y;\theta)$表示由$\theta$确定的在相同空间上的概率分布，$p_{data}(y)$表示$y$的真实概率分布。

$p_{model}(y;\theta)$将任意输入映射到实数来估计真实概率分布$p_{data}(y)$。

对$\theta$的最大似然估计定义为（N为样本个数）：
$$
\theta_{ML}=argmax_\theta\prod_{i=1}^Np_{model}(y_i;\theta)
$$
对数似然形式：
$$
\theta_{ML}=argmax_\theta\sum_{i=1}^Nlog(p_{model}(y_i;\theta))
$$
最大似然估计的过程是最小化训练集上经验分布$\hat{p}_{data}$和模型分布$p_{model}$之间的差异，即最小化KL散度，实际上为最小化分布之间的交叉熵，即下式：
$$
-E_{x\sim\hat{p}_{data}}[log(p_{model}(x))]
$$
而当**误差服从均值为0的高斯分布**时，有：
$$
\epsilon\sim N(0,\sigma)
$$

$$
y\sim N(\hat{y},\sigma)
$$

模型分布$p_{model}(y)$可表示为：
$$
p_{model}(y)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\hat{y})^2}{2\sigma^2})
$$
其对数似然为：
$$
\begin{align}
log(p_{model}(y))&=log(\frac{1}{\sqrt{2\pi}\sigma})-\frac{(y-\hat{y})^2}{2\sigma^2}\\
&=-log(\sqrt{2\pi}\sigma)-\frac{(y-\hat{y})^2}{2\sigma^2}
\end{align}
$$


则有：
$$
-E_{x\sim\hat{p}_{data}}[log(p_{model}(x))]=-(-log(\sqrt{2\pi}\sigma)-\frac{1}{2\sigma^2}\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y})^2)=log(\sqrt{2\pi}\sigma)+\frac{1}{2\sigma^2}\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y})^2
$$
该式前项为常数，后项为最小二乘法中的公式，极大似然估计中最小化交叉熵的公式与最小二乘估计等价。



但若**误差不服从均值为0的高斯分布**时，极大似然估计与最小二乘估计不等价，其中一个例子就是在**逻辑回归**中不使用最小二乘做损失函数，因为逻辑回归与最小二乘的$y$服从不同的概率分布 ，最小二乘服从正态分布，而逻辑回归服从二项分布（$y$非0即1），即$y\sim Bernoulli(\phi)$。



